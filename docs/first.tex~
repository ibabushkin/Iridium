% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

\documentclass[11pt]{article}

\usepackage[utf8]{inputenc}
\usepackage{natbib}
\usepackage{multicol}

\usepackage{geometry}
\geometry{a4paper,left=3.5cm,right=2.5cm}

\usepackage{graphicx}

\usepackage{booktabs}
\usepackage{array}
\usepackage{paralist}
\usepackage{verbatim}
\usepackage{subfig}

\usepackage{fancyhdr}
\pagestyle{fancy}
\renewcommand{\headrulewidth}{0pt}
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\thepage}\rfoot{}

\usepackage{sectsty}
\allsectionsfont{\sffamily\mdseries\upshape}

\usepackage[nottoc,notlof,notlot]{tocbibind}
\usepackage[titles,subfigure]{tocloft}
\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape}

\usepackage{dirtree}
\columnseprule0.01pt
\columnsep7mm

\usepackage{amsmath, amssymb, amstext, amsfonts, mathrsfs, url}
\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
\renewcommand{\bibname}{Bibliotheksreferenz}
\renewcommand{\contentsname}{Inhalt}
\renewcommand{\figurename}{Figur}

\title{Besondere Lernleistung - Dokumentation}
\author{Inokentiy Babushkin}
\date{}

\begin{document}
\maketitle
\tableofcontents

\section{Grundlagen}
Die vorliegende Besondere Lernleistung hat das Ziel, ein Softwareprojekt
umfassend zu planen und umzusetzen.
Dieses Dokument beschreibt Zielsetzung, Aufbau und konkrete Aspekte der Umsetzung,
wobei der Hauptaugenmerk auf Algorithmen, Vorgehensweisen und Designentscheidungen liegen wird.

Bei besagtem Softwareprojekt handelt es sich um ein System, welches Reverse-Engineering, manuelle
Dekompilation und bestimmte Aspekte der Malware-Analyse vereinfachen bzw. automatisieren soll,
ohne dass durch seine Benutzung weitere Probleme entstehen. Folglich lag der Schwerpunkt der Entwicklung
auf einem sinnvolen Programmdesign, effizienten und effektiven Algorithmen und einer guten Grundlage
für Erweiterungen aller Art. Aus diesem Grund ergaben sich mehrere Entscheidungen, die den Entwicklungsprozess
und die Eigenschaften des Projekts nachhaltig prägten.
\begin{enumerate}
	\item{Es sollte eine Programmiersprache verwendet werden, die dem potentiellen Nutzer mit hoher
		Wahrscheinlichkeit zur Verfügung steht, die er/sie ggf. bereits beherrscht und die nicht nur
		einfach zu schreiben, sondern auch ebenso einfach zu lesen ist. Zusätzlich sollte sie Erweiterungen
		durch den Nutzer möglich machen. Deswegen fiel die Wahl auf Python, eine lesbarkeitsbetonte,
		interpretierte höhere Programmiersprache, die multiple Paradigmen unterstützt.
		Dabei beeinflusste die vorherige Erfahrung des Autors diese Entscheidung wesentlich.
	}
	\item{Das Projekt sollte freie Software werden. Diese Entscheidung war nicht nur ideologisch begründet;
		Vielmehr kann die Qualität des Quellcodes und dementsprechend der Arbeitsergebnisse der Software
        auf diese Weise wesentlich besser kontrolliert werden, während die Gesellschaft bzw. ein Teil von
        ihr auf diese Art deutlich besser von der Arbeit des Autors profitiert. Aus diesem Grunde wurde die
        GNU General Public License Version 3 (GPL v3) verwendet, die die Interessen des Autors in dieser Hinsicht
        am besten vertritt.
	}
\end{enumerate}

\section{Eine kurze Einführung in Reverse Engineering und Dekompilierung}
\subsection{Grundlegende Funktionsweise von CPU und die Abstraktion der Assemblersprache}\footnote{
		Die hier beschriebene Syntax ist die Intel-Syntax, die unter der x86(-64)
		Architektur am weitesten verbreitet ist. Zudem beschränkt sich das gesamte Projekt derzeit
        auf die x86(-64) Systemarchitektur.}
Die CPU, also der Prozessor eienes Computersystems unterteilt jedes mögliche Programm in sogenannte
Instruktionen, die die kleinste unteilbare Einheit eines Programms darstellen. Es werden
verschiedene Instruktionsklassen unterschieden. Allgemein ist jede Instruktion nach folgendem Muster
aufgebaut: Sie besteht aus 2 bis 3 Tokens (Wörtern), wobei der erste den Namen der Instruktion, der
diese eindeutig kennzeichnet, darstellt. Im Assembler-Jargon wird dieser als \textit{Mnemonic}
bezeichnet. Die folgenden 1 bis 2 Tokens sind sogenannte \textit{Operanden}, die noch weiter in
\textit{Source-} und \textit{Destination-} Operand unterteilt werden. Diese sind immer
eine Referenz auf Daten, auf die die Instruktion angewandt wird. Beispielsweise gibt es
Instruktionen für Grundrechenarten und bitlogische Operationen, wobei folgende Darstellungsform
verwendet wird: $$ mnemonic\ dest, src$$
\subsubsection{Einige häufig anzutreffende Instruktionen}
\begin{itemize}
	\item{\textit{add:}} Addiert Source- und Destination-Operand und speichert das Ergebnis in
		letzterem.
	\item{\textit{sub:}} Subtrahiert den Source- vom Destination Operand und speichert in letzterem
		das Ergebnis.
	\item{\textit{mov:}} Kopiert die Daten aus dem Source- in den Destination-Operand.
	\item{\textit{push:}} Akzeptiert nur einen Source-Operanden und legt diesen auf dem Stack
      \footnote{Der Stack ist ein Speicherbereich, der wie die gleichnamige Datenstrukturen aufgebaut
        ist und auf dem Argumente an Funktionen, Rückkehradressen und lokale Variablen so gespeichert
        werden, dass nur die entsprechenden Programmteile darauf zugreifen können.}
      ab.
	\item{\textit{pop:}} Akzeptiert nur einen Destination-Operanden und legt in ihm das oberste
		Element auf dem Stack ab, wobei dessen Größe im Operanden definiert wird.
		Wenn also ein 4 Byte großer Speicherbereich den Destination-Operanden darstellt,
		werden die obersten 4 Byte auf dem Stack in ihm abgelegt und der Stack
		entsprechend verkleinert.
	\item{\textit{cmp:}} Führt einen Vergleich der Operanden durch und setzt entsprechende
		CPU-Flags. Diese sind in einem speziellen EFLAGS-Register
		\footnote{
			EFLAGS steht für "Extended Flags" und bedeutet, dass das Register 32 boolesche
			Werte mit klar umrissener Bedeutung enthält, während das FLAGS-Register, welches
			unter der 16-Bit CPUArchitektur üblich war, nur 16 Bit umfasste.
		}zu finden und sind
		Boolsche Werte, die über das Ergebnis von Vergleichen und anderen Instruktion (\textit{sub},
		\textit{add}, ...)Auskunft geben. Es existieren zudem eine Reihe von Instruktionen,
		die in Abhängigkeit vom Zustand bestimmter Flags ausgeführt werden.
	\item{\textit{Sprünge:}} Der einfachste Sprung, \textit{jmp}, wird immer ausgeführt und
		springt zur Adresse / zum Label in seinem einzigen Operanden. Alle anderen
		Sprunginstruktionen fragen vor der tatsächlichen Sprunginstruktion die CPU-Flags ab,
		sind also sogenannte \textit{bedingte Sprünge}. Deren Instruktions-Mnemonics fangen
		in der Regel mit einem \textit{j} an.
	\item{\textit{call}} Ruft die durch den einzigen Operanden identifizierte Funktion auf.
\end{itemize}
Diese Liste ist natürlich bei weitem nicht vollständig, da sie nur dazu gedacht ist, die dem
Programm beigefügten Test-Cases und Beispiele für vollkommene Neulinge verständlicher zu gestalten.
Um ein umfassendes Grundwissen über Programmierung und Verständnis von der Assemblersprache (zwecks
Reverse-Engineering\footnote{Der Prozess, Software oder andere technische Systeme in ihrem
Aufbau und Verhalten zu analysieren, um sie zu reproduzieren, zu modifizieren oder das dabei
gewonnene Wissen weiterzuverwenden, z.B. um Anti-Malware Routinen zu entwickeln}) zu erlangen,
empfehlen sich die unter den Literaturverweisen \cite{wikibook1:7} und \cite{wikibook2:8}
referenzierten Wikibooks.
\subsubsection{Funktionen}
Zum weiteren Verständnis des Projekts ist eine Erklärung von Funktionen in der Assemblersprache vonnöten.
Eine Funktion ist, genau wie in Hochsprachen, ein Codeabschnitt, der blockweise ausgeführt wird,
wobei innerhalb eines solchen Blocks Verzweigungen möglich sind. Eine Funktion kann Argumente
(Parameter) annehmen, welche für ihre Arbeit notwendig sind und meistens über den Stack
übergeben werden.

Auch kann eine Funktion Variablen mit lokal begrenzter Gültigkeit auf dem eigenen
Stackframe, dem für jede aufgerufene Instanz individuellen Stackabschnitt, ablegen.
Dabei muss jede Funktion ihren Stackframe selbst initialisieren und - je nach Aufrufkonvention -
wieder deinitialisieren. Gleichzeitig legt die Definition der Funktion die Reihenfolge der Parameter
auf dem Stack fest. Auch dies ist nur eine sehr kurze Einführung in die grundlegenden Konzepte, die
für das Verständnis von Assembler-Programmen relevant sind, deswegen sei nochmal auf
\cite{wikibook1:7} und \cite{wikibook2:8} verwiesen.

\subsection{Compiler, Decompiler und Disassembler}
Jede Hochsprache, die bis jetzt entwickelt wurde, hatte das Ziel, dem Programmierer ein effizientes
Werkzeug bereitzustellen, welches verhältnismäßig einfache Softwareentwicklung möglich machen
sollte, ohne ihn in seiner Handlungsfreiheit zu beschränken. Gleichzeitig soll der gesamte Prozess
der Entwicklung für den Programmierer so transparent wie möglich sein.

Tatsächlich hat sich dieser Ansatz bewährt und erfüllt die Zielvorgaben. Damit die in einer solchen
Sprache formalisierten Algorithmen auch von einem Prozessor ausgeführt werden können, muss der Quellcode
erst in eine für den Computer angemessene Form überführt werden, das heißt in die Assemblersprache
beziehungsweise ihre Representation als einzelne Bytes im Speicher. Genau dies ist die Aufgabe eines
sogenannten Compilers: Dieses System nimmt Hochsprachen-Quellcode als Eingabe und gibt entweder
Assemblerprogramme oder direkt ausführbare Binärdateien zurück, die das Eingabeprogramm
repräsentieren. Dabei verlieren die so umgewandelten Programme einige Informationen, die für
die Ausführung unerheblich sind, zu ihrem Verständis jedoch entscheidend beitragen: Kommentare,
Klassenprototypen, Variablennamen und viele weitere Details, die das Lesen von Quellcode durch menschliche
Nutzer wesentlich vereinfachen. Wenn ein Programm jedoch nur in Binärform vorliegt,
ist die Analyse - je nach Komplexität des Analyseziels - kompliziert und
erfordert fundierte Fachkenntnisse über die Funktionsweise der CPU und grundlegende Konzepte der
Assemblerprogrammierung, zumal die erwähnten Vereinfachungen und Hilfestellungen der Hochsprachen
nicht verfügbar sind. Stattdessen besitzt die Assemblersprache - wie im vorangehenden Abschnitt
erläutert - keine Kontrollstrukturen im eigentlichen Sinn, was es schwierig macht,
den Kontrollfluss eines Programms, das in dieser Form vorliegt, zu verstehen.

Die tatsächliche
Umwandlung einer Binärdatei in ein Assemblerlisting, welches grundlegende Abstraktionen über
einzelne Instruktion der CPU bereiststellt, ist Aufgabe eines Disassemblers, der aus diesem Grund
ein zentrales Wekzeug eines Reverse Engineers darstellt. Durch die allgemeine Ausrichtung eines
Compilers, nämlich schnell und effizient arbeitenden Code zu generieren und dabei auf (hürdenfreie)
Menschenlesbarkeit unter Umständen zu verzichten, wird diese Art der Analyse zusehends zeitintensiver.
Aus diesem Grund gibt es auch einige Ansätze, den Dekompilierungsprozess komplett zu automatisieren, so dass
keine oder kaum Nutzerinteraktionen notwendig sind wovon sich die Entwickler dieser System eine enorme
Beschleunigung des Prozesses erhoffen. Diese technischen Lösungen sind als Decompiler bekannt, da sie
- im Idealfall - die Funktionsweise eines Compilers umkehren. Da diese meist über eine für ein Programm
überaus komplexe Aufgabe erfüllen, sind es auch in der Regel kommerzielle Produkte, die auf diesem
Gebiet verhältnismäßig weite Verbreitung finden, wobei auch eine Anzahl von Open-Source-Projekten
sich mit dieser Zielsetzung befassen.

\section{Problematik}
Das erklärte Ziel dieses Projekts ist es, ein Programm zu entwickeln, welches
es ermöglicht, Software teilweise automatisch zu dekompilieren. Da moderne Software immer komplexer
wird, wird es entsprechend schwieriger, in binärer Form vorliegende Softwareprodukte so umzuformen,
dass sie ein tiefgehendes Verständnis und darauf basierende Modifikation erlauben.
Dies betrifft nicht nur die Untersuchung von fremder Software, sondern auch die Wiederherstellung
von Quellcode aus alten, beendeten oder sonstwie des Quellcodes verlustig gewordenen Projekten.
Ein weiteres wichtiges Themenfeld ist die Malware-Analyse
\footnote{
	Malware ist ein Oberbegriff für alle Arten von Schadprogrammen, die gegen den Willen des Nutzers
	auf seinem Rechner installiert werden. Malware-Analyse ist demenstprechend die Untersuchung solcher
	Programme und die Entwicklung von Gegenmaßnahmen.},
die Schadsoftware untersucht und die von
den Malware-Autoren unternommenen Versuche zur Codeverschleierung, Obfuskation genannt, umgehen muss.
Dies ist recht aufwändig, allerdings nicht minder notwendig, da nur auf diese Weise Gegenmaßnahmen unternommen
werden können, die von simplen Desinfektionsroutinen bis hin zu komplexen Erkennungsalgorithmen und
Signatursystemen reichen.

Ein weiterer Aspekt, der diese Aufgabenstellung so bedeutend macht, ist die Tatsache,
dass Malware schon seit einiger Zeit eine enorme Bedrohung nicht nur für die Computer privater Nutzer, sondern
auch für Unternehmen und staatliche Einrichtungen darstellt. Zusammenfassend lässt sich die Situation
folgendermaßen beschreiben: Es existiert eine Vielzahl von sinnvollen und gegebenenfalls lukrativen
Anwendungsmöglichkeiten für zumindest teilweise automatisierte Programmanalyse. Gleichzeitig sind die
Produkte, die die daraus resultierende Nachfrage abdecken sollen, für einige der Nischen nicht oder nur
wenig geeignet, was einen anderen Ansatz zur Lösung der Problemstellung erfordert, der auch im vorliegenden
Projekt verfolgt werden soll. Allerdings ist an dieser Stelle eine grundsätzliche Erläuterung der Methoden,
die bereits existierender Software des Themenfelds zugrundeliegt, angemessen.

Die Hauptargumente für eine automatisierte Vorgehensweise sind, dass die Wiederherstellung von Kontrollfluss,
Variablen, Funktionen und weiteren HLL\footnote{High Level Language, also Programmiersprache mit hohem
Abstraktionsgrad, bekannte Beispiele für kompilierte HLL's sind C, C++ und Java}-spezifischen Strukturen
ohnehin nicht komplett möglich ist, das Ergebnis also bloß eine Annäherung an den Originalcode sein kann.
Gleichzeitig ist der Vorgang - von Menschen durchgeführt - sehr zeitraubend, erfordert viel
Fachwissen und möglichst detaillierte Kenntnisse über die Funktionsweise der zu untersuchenden Software.
All dies lässt es vorteilhaft erscheinen, die recht schwierige und aufwändige  Aufgabe des
Reverse-Engineerings auf ein automatisiertes System zu verlagern, vorausgesetzt das Ziel der Untersuchung
ist der komplette Quellcode des zu untersuchenden Produkts. Diese Systeme beruhen auf verschiedenen
theoretischen Ansätzen, Programme zu beschreiben und Muster in ihnen zu erkennen, die in ihrer Komplexität
stark variieren. Dabei sind mehrere Programmkategorien zu unterscheiden:
\begin{enumerate}
	\item{Decompiler, deren Ziel es ist, ein Programm
		komplett wiederherzustellen, die meistens eine non-interaktive Arbeitsweise erfordern.}
	\item{Programme, die bestimmte Aspekte des zu analysierenden Codes wiederherstellen oder eine Art
		Anleitung für den Benutzer generieren, wie er am besten verfährt, um ein umfassendes Verständnis
		vom zu untersuchenden System zu erlangen. Diese Definition ist absichtlich sehr weit gefasst, da
        die Produkte aus dieser Kategorie in ihren Eigenschaften sehr stark variieren.
      }
\end{enumerate}

Das Resultat der vorliegenden Besonderen Lernleistung ist in
die zweite Kategorie einzuordnen, wobei es aus drei Submodulen besteht, die unabhängig arbeiten und
verschiedene Bereiche des Analysegebiets abdecken. Die Entscheidung, keinen kompletten Decompiler
anzustreben wurde aus mehreren Gründen getroffen: Zum Einen ist automatisierte Programmanalyse recht
genau, kann jedoch trotz allem Fehler erzeugen. Deswegen ist es deutlich günstiger, eine
transparente Arbeitsweise und eine enge Zusammenarbeit zwischen Benutzer und System zu ermöglichen,
da auf diese Weise Fehler deutlich schneller gefunden und behoben werden können, was die Qualität
der Arbeit des Nutzers wesentlich verbessert und ihm eine enorme Zeitersparnis ermöglicht. Zum Anderen sind
viele Funktionen und Features in einem solchen Projekt machbar und können auch mehr oder weniger
einfach integriert werden, obwohl sie kaum Nutzen erbringen, da das Verhältnis zwischen
Ergebnisqualität und Programmkomplexität zu schlecht ist und die erbrachten Resultate zu fragil.

Folglich ist es sinnvoll, sich auf die wesentlichen Aufgabenstellungen des Dekompilierungsprozesses
zu beschränken, so dass der Aufwand in Relation zum erbrachten Nutzen steht. Auf diese Weise kann
gewährleistet werden, dass solche Systeme dem Nutzer tatsächlich bei einer Aufgabenstellung helfen,
und ihn nicht in einigen Fällen, für die das System nicht ausgelegt war, wesentlich behindern, wie es
der Fall sein könnte, wenn eine andere Entwicklungsphilosophie angewendet werden würde.

\section{Zielsetzung} Wie bereits im letzten Abschnitt erwähnt, handelt es sich bei diesem Projekt
um ein Analyseframework zur teilweisen Dekompilierung von Software, wobei auch eine Vielzahl
weiterer Reverse-Engineering-Aufgaben mit seiner
Hilfe vereinfacht werden können. Dabei wird die Ausgabe von IDA Pro
\footnote{IDA Pro (Interactive Disassembler)
	ist ein Disassembler, der
	viele verschiedene Eingabeformate unterstützt und nicht nur interaktive Arbeit ermöglicht, sondern
	auch Assemblerlistings etc. generieren kann.}
als Grundlage der Analyse verwendet. Dies mag zuerst
als Nachteil erscheinen, da es den Arbeitsverlauf des Nutzers einschränkt, wobei bedacht werden
muss, dass der Analyse- und Input-Parsing-Code strikt getrennt wird, was es ermöglicht, andere
Eingabeformate mithilfe weiterer Module zu akzeptieren, ohne Änderungen in der Programmmechanik
umsetzen zu müssen. Die Umsetzung des Moduls für IDA wurde zuerst durchgeführt, da diese keine
wesentliche Verarbeitung der Eingabe erfordert, um die für die Programmmechanik notwendigen
Informationen bereitzustellen, weswegen ein solches Modul sich besonders gut als
Demonstrationsbeispiel eignet. Für mehr Informationen zum Projektaufbau und eine graphische
Darstellung sei auf den gleichnamigen Abschnitt verwiesen.

\subsection{Projektstruktur} Um eine Erweiterung im Sinne des vorhergehenden Abschnittes zu
gewährleisten, ist es besonders wichtig, eine sinnvolle Aufteilung des Codes in einzelne Module
durchzuführen. Auch sonst gehört es zu einem  "guten Stil", Programme in Sinneinheiten zu koppeln.
Wie bereits erwähnt, wird deswegen das Projekt grob in zwei zentrale Teile unterteilt: Die
tatsächlichen Analysemodule, die alle eine Eingabe in einem bestimmten Format erwarten, und Module,
die den Zweck erfüllen, Daten in diesem Format bereitzustellen und diese aus verschiedenen
Eingabeformaten generieren. Die Ordnerstruktur ist in Figur 1 dargestellt und wird
primär verwendet, um das Projekt zu strukturieren. An der Wurzel der Ordnerstruktur liegen die
einzelnen Module, die Eingabe verarbeiten und in ein Format umwandeln, welches von den Analysemodulen
verwendet wird. Diese wiederum befinden sich innerhalb des Moduls \textit{defines}, welches pro
Submodul je einen Unterordner besitzt. Jedes dieser Submodule kann sowohl alleinstehend, also als
Script, als auch als Modul benutzt bzw. importiert werden. Auf diese Weise sind die
einzelnen Komponenten fähig, miteinander in Kontakt zu treten und auf die Funktionalität anderer
Module zuzugreifen, jedoch ohne zu sehr voneinander abhängig zu sein, so dass Erweiterung und
Modifikation sehr einfach zu bewerkstelligen sind. Beispielsweise sind so ohne Weiteres andere
Eingabeparsing-Module denkbar, als die bereits mitgelieferten. Ebenfalls kann die Analyse um weitere
Aspekte des zu untersuchenden Programms erweitert werden, sollte dies sich als notwendig
herausstellen.
\begin{figure}[h!]
		\dirtree{%
		.1 Iridium (Eingabeparsing-Module).
		.2 defines (Behälter für Submodule).
		.3 cfg (Kontrollflussanalyse).
		.3 data (Bestimmung von Variablen etc).
		.3 div (Analyse optimisierter Divisionen).
		.3 util (Parent-Klassen etc).
		.2 docs (Dokumentation, wie dieses Dokument).
		.2 tests (Testdateien und -eingaben).
		}
	\caption{Die Ordnerstruktur des Projekts}
\end{figure}
\subsection{Kontrollflussanalyse} \label{sec:CFA} Einer der zentralen Aspekte moderner wie
historischer Programme ist die Möglichkeit, Code in Abhängigkeit von Bedingungen ausführen zu
können. Die am weitesten bekannten Beispiele sind verschiedene Arten Bedingter Anweisungen, Schleifen
u.ä. Diese Beziehungen zwischen den einzelnen Teilen des Codes wiederherstellen zu können ist eine der
Hauptaufgaben dieses Projekts. Dabei ist diese Zielsetzung keineswegs trivial, zumal Strukturen
linear und nicht linear verschachtelt\footnote{Strukturen in einem CFG (Kontrollflussgraph)
bestehen in der Regel aus zwei oder mehr Teilen, wobei sogenannte primitive Teile einfache Knoten
des Kontrollflussgraphen sind. In der Praxis können jedoch auch nicht-primitive Teile, also andere
Strukturen an dieser Stelle auftreten. Wenn eine Struktur komplett eine andere Teilstruktur ersetzt,
wird der Begriff "linear verschachtelt" verwendet. Gleichzeitig ist es möglich, dass Strukturen
verschiedener Ebenen den gleichen Knoten verwenden, was als nichtlineare Verschachtelung bezeichnet
wird. Beide Begriffe sind vom Autor selbst geprägt.} sein können. Als Grundlage des verwendeten
Algorithmus diente die in \cite{structural_analysis:1} beschriebene Vorgehensweise, die jedoch um
fortgeschrittenere Bedingungsanalyse erweitert wurde. Dies war notwendig, da z.B. Bedingte Anweisungen
nicht nur eine, sondern mehrere über logische Operatoren verknüpfte Bedingungen besitzen können.
Dies wiederum führt dazu, dass Strukturen erst erkannt werden können, wenn diese komplexen Bedingungen
zu einem Knoten reduziert wurden.

\subsection{Analyse von Variablen und anderen Daten} Daten bilden neben dem Code den zweiten
Hauptbestandteil eines Programms, weswegen das Layout der Variablen, Argumente/Parameter und
Rückgabewerte von großem Interesse für den Reverse-Engineer sein sollte. Gleichzeitig sind Arrays
und Variablen nicht immer einfach zu erkennen, weswegen sich einige einfache, dennoch effektive
Heuristiken anbieten, die in diesem Projekt umfassend realisiert wurden. Die eigentliche Problematik
liegt darin, dass Compiler nicht alle Daten auf dem Stack ablegen, aber gleichzeitig den Stack nicht
nur zum Speichern lokaler Variablen verwenden. Folglich muss unterschieden werden, welche
Speicherbereiche Variablen abbilden, welchen Datentyp diese besitzen und ob diese gegebenenfalls
Pointer darstellen. Aufgrund der oben beschrieben Umstände handelt es sich bei den
Analyseergebnissen nur um eine Annäherung an die Realität, beispielsweise verwenden optimisierende
Compiler zum Speichern lokaler Variablen häufig CPU-Register
\footnote{Ein Speicherbereich, der direkt an die CPU angeschlossen ist und sehr viel schnelleren
Zugriff erlaubt, allerdings in seiner Größe und Anzahl begrenzt ist. Jede Variation der
Assemblersprache definiert eine bestimmte Anzahl sogenannter Universalregister, die auf bestimmte
Weise benannt werden und spezifisch für die jeweilige Prozessorarchitektur sind. Für die
x86-Prozessorarchitektur sind dies EAX, EBX, ECX, EDX, ESI, EDI, EBP und ESP. Diese sind 32 Bit
breit, fassen also 4 Byte und können auch teilweise angesprochen werden. Dabei sind EBP und ESP mit
einer besonderen Bedeutung versehen: Sie zeigen auf Anfang und Ende des aktuellen Stackframes und
können nicht vom Programm anderweitig benutzt werden.}
, die von dem vorliegenden
Modul nicht berücksichtigt werden, da so viel Arbeit in zweifelhafte Ergebnisse investiert
werden würde, zumal Register auch für Arithmetik und Argumentübergabe verwendet werden können.

\subsection{Behandlung von Integerdivision} Die Integerdivision ist, ebenso wie die Modulodivision,
eine verhältnismäßig häufig verwendete Rechenarten in Programmen, bzw. ist meistens von zentraler
Bedeutung für die Resultate der Ausführung. Allerdings sind diese Operationen auch sehr
ressourcenintensiv, weswegen seit den späten 1990er Jahren die entsprechenden Prozessorinstruktionen
kaum noch verwendet wurden, zumal die Division mit konstantem Divisor durch  Multiplikation und Shifts
ersetzt werden kann, was deutlich schnellere Ausführung ermöglicht\cite{division:2}, wodurch
gerade Schleifen beschleunigt werden können. Gleichzeitig wird dadurch die eigentliche
Bedeutung des Codes verschleiert, was zu Hürden bei der Analyse durch Menschen führt. Auch hier ist
eine automatisierte Lösung also naheliegend, zudem noch nicht allgemein verbreitet\footnote{Eine
Recherche des Autors ergab, dass z.B. der Hex-Rays Decompiler "ohne Probleme" mit solchen
Konstrukten umgehen kann, da dieser allerdings ein komerzielles, recht teures Produkt
darstellt, kann vermutlich nicht von allgemeiner Verbreitung gesprochen werden.}. Gleichzeitig ist
die Berechnung des Divisors aus den vorliegenden Konstanten durchaus möglich und ausreichend
beschrieben bzw. erklärt\cite{stackexchange:4}.

\section{Umsetzung} Im Folgenden werden die verwendeten Algorithmen im Detail beschrieben,
auch Informationen zur Implementation sind hier wiedergegeben.

\subsection{Kontrollflussanalyse} Wie bereits in Abschnitt \ref{sec:CFA} beschrieben, ist die
Kontrollflussanalyse eine verhältnismäßig komplexe Aufgabe, da das zu untersuchende Objekt eine
Vielzahl an Formen annehmen kann, beispielsweise arrangieren Compiler den generierten Assemblercode
nicht immer auf eine direkt zu erwartende Weise. Aus diesem Grund wurde im Laufe der Planung
vorliegender Software sehr schnell klar, dass die Analyse des Programms auf Codeebene nicht
erfolgreich sein kann, da die Strukturen kaum noch erkannt werden, wenn sie verschachtelt vorliegen.
Eine Untersuchung der Fachliteratur ließ erkennen, dass die Analyse des Kontrollflusses deutlich
effektiver ist, wenn dieser als Graph betrachtet wird. Per Definition ist ein Kontrollfussgraph
(CFG) ein gerichteter Graph \(G\), der über eine Menge Knoten \(V\), eine Menge gerichteter Kanten
\(E\) und einen Wurzelknoten \(r\) mit \(r \in V\) verfügt \cite{wiki1:3}. \[G(V,E,r)\] Aufgrund der
Konzeption von Codeflusssteuerung auf Maschienenebene kann zudem davon ausgegangen werden, dass jeder
Knoten 0 bis 2 Nachfolger hat, allerdings eine beliebige Anzahl Vorgänger, wobei nur der
Wurzelknoten keinen Vorgänger haben darf. Die Verwendung einer solchen Datenstruktur zur
Representation des Programms hat eine Reihe von Vorteilen, wobei die wichtigsten eine deutlich
einfachere Analyse möglicher Pfade vom Start- zum Endknoten und die Unabhängigkeit vom Codelayout
durch den Compiler im Speicher sind. Die Software geht bei der Analyse folgendermaßen vor: Die
Knoten werden nacheinander bearbeitet, wobei die Reihenfolge der Postorder-Traversierung des
Tiefensuchbaumes des Graphen entspricht. Ist der aktuelle Knoten der Anfang einer simplen Struktur,
die es zu erkennen gilt, werden die dazugehörigen Knoten und Kanten extrahiert und durch einen
Knoten ersetzt, der Informationen über die enthaltene Struktur speichert, und welcher als nächstes
analysiert wird. Sobald alle Knoten auf diese Weise behandelt wurden und alle notwendigen
Graphenreduktionen stattgefunden haben, können die Informationen über erkannte Schleifen,
Verzweigungen etc. in strukturierter Form dargestellt werden. Das aktuelle Featureset umfasst vor-
und nachprüfende Schleifen, Verzweigungen mit beliebig vielen alternativen Pfaden und alle 
Möglichkeiten der Verschachtelung. Seit neuestem werden auch komplexe Bedingungen in allen
möglichen Kontexten erkannt und reduziert, was ein Novum darstellt, da der dazu verwendete
Algorithmus bis jetzt nicht in der dem Autor bekannten Fachliteratur beschrieben wurde. Sprungbefehle
wie goto, break, continue und in manchen Fällen return werden aufgrund der Herangehensweise nicht
erkannt, wobei eine entsprechende Änderung in zukünftigen Versionen durchaus möglich (und sinnvoll) ist.

\subsubsection{Algorithmusbeschreibung und Implementationsdetails} Das Programm lässt sich in
mehrere Abschnitte unterteilen, die sich verschiedenen Phasen der Analyse zuordnen lassen:
Generieren des Graphen und die Analyse des Graphen selbst, die sich wiederum in zwei Abschnitte
unterteilbar ist: Bestimmung der Analysereihenfolge der Knoten und die Analyse jedes einzelnen
Knotens nach dieser Reihenfolge.
\begin{enumerate}
	\item{\textbf{Generierung des Graphen aus Assemblercode:}}
		Die Unterteilung des Assemblerlistings erfolgt an sogenannten Trennpunkten
		zwischen Codezeilen.
		Diese erfüllen mindstens eine der nachfolgenden Bedingungen:
		\begin{enumerate}
			\item{Die vorherige Instruktion ist ein bedingter oder unbedingter Sprung.}
			\item{Die nachfolgende Instruktion ist mit einem Label versehen,
				also Ziel eines oder mehrer Sprünge.}
			\item{Die nachfolgende Instruktion ist ein Vergleich bzw. ist primär dazu da,
                die CPU-Flags zu setzen.}
		\end{enumerate}
		Wenn auf diese Weise alle Knoten des Graphen bestimmt sind, werden mögliche Übergänge von
		Knoten zu Knoten, also Kanten des Graphen, generiert, die \textit{aktiv} oder
		\textit{passiv} sein können. \textit{Aktive} Kanten sind Kanten, die
		tatsächlich ausgeführten Sprüngen entsprechen, während \textit{passive}
		Kanten keinem Sprung entsprechen, sondern einem Übergang zum nächsten Befehl,
		der in einem anderen Knoten liegt.
	\item{\textbf{Bestimmung der Analysereihenfolge für die Knoten des Graphen:}} Hierfür wird
		zuerst der Tiefensuchbaum des in Schritt 1 erstellten Graphen berechnet.\footnote{
			Ein Tiefensuchbaum oder Depth-First-Search-Tree (DFS-Tree) wird generiert,
			indem ein Graph per Tiefensuche traversiert wird und die dabei besuchten
			Knoten auf eine bestimmte Weise in einem  Baum abgelegt werden. Eine genauere
			Beschreibung des Algorithmus kann unter \cite{wiki2:5} eingesehen werden.
		}
		Dieser wird dann per Postorder\footnote{
			Die Postorder-Traversierung ist eine Methode, für einen Baum eine eindeutige
			Reihenfolge der Knoten zu bestimmen. Dabei handelt es sich um einen
			rekursiven Algorithmus, der auf jeden Knoten im Baum angewendet
			wird. Dabei wird das Ergebnis der Postorder-Traversierung der Kindknoten
			zuerst an die Liste der Knoten angefügt, danach der eigentliche Knoten. Auf
			diese Weise wird der ganze Baum durchlaufen, wobei die Wurzel immer das
			letzte Element der Ergebnismenge darstellt. Mehr Details sind unter
			\cite{wiki3:6} zu finden.
		}
		traversiert. Die sich dabei ergebende Reihenfolge ist
		für die Analyse sehr gut geeignet, da auf diese Weise die innersten Strukturen
		zuerst erkannt und reduziert werden, sodass kein Knoten, der am Anfang einer
		Struktur steht, analysiert wird, während der Graph eine Erkennung der entsprechenden
		Struktur unmöglich macht, da Teile der Struktur noch weitere unreduzierte Strukturen
		enthalten, wie es im Falle von verschachtelten Bedingten Anweisungen und / oder Schleifen
		der Fall sein kann, wenn eine andere Analysereihenfolge verwendet werden würde.
	\item{\textbf{Analyse der einzelnen Knoten nach der in Schritt 2 bestimmten Reihenfolge:}}
		Zuerst wird überprüft, ob der aktuelle Knoten Anfang einer simplen
		Struktur sein kann. Als simple Strukturen werden alle elementaren
		Konstrukte bezeichnet, die von Hochsprachen definiert werden: vor-
		und nachprüfende Schleifen, lineare Abfolgen beliebiger Strukturen
		und if-then / if-then-else -strukturen, also Bedingte Anweisungen.
		Das Programm unterscheidet die folgenden Strukturen:
		\begin{enumerate}
			\item{\textit{if-then:}}
				Eine bedingte Anweisung, ohne else-Klausel.
			\item{\textit{if-then-else:}}
				Eine bedingte Anweisung, mit else-Klausel.\footnote{
					Bedingte Anweisungen, die mindestens ein \textit{else if}
					enthalten, werden als verschachtelte \textit{if-then-else}
					bzw. \textit{if-then}-Srukturen angesehen, da dies die
					Analyse deutlich vereinfacht und zudem für Compiler
					keinen Unterschied darstellt.
				}
			\item{\textit{while-loop:}}
				Eine vorprüfende Schleife, entspricht sowohl der
				while- als auch der for-Schleife, da diese vom
                Compiler im Grunde genommen identisch behandelt werden.
			\item{\textit{do-loop:}}
				Eine nachprüfende Schleife.
			\item{\textit{condition:}}
				Eine komplexe Bedingung.
			\item{\textit{block:}}
				Eine beliebig lange, lineare Abfolge von sowohl
				primitiven, als auch komplexen Knoten, ausgenommen
				Bedingungen.\footnote{
					Tatsächlich kann es vorkommen, dass Bedingungen direkt in
					einem Block platziert werden, wenn dieser Block der einzige
					Knoten in einer nachprüfenden Schleife ist.
					Dies liegt an der Art, auf die nachprüfende Schleifen
					erkannt werden und wird \textit{nicht} als Bug angesehen,
					da dieses Verhalten kaum zu beheben ist, allerdings auch
					nicht seht störend ist.
				}
		\end{enumerate}
		Ist eine solche Sruktur aufzufinden, werden ihre Teile aus dem Graphen entfernt
        und es wird an entsprechender Stelle ein Knoten in den Graphen eingefügt, der
		die Informationen über sie speichert. Sollte dies jedoch nicht der Fall
		sein, wird überprüft, ob zuerst eine komplexe Bedingung reduziert
		werden kann. Falls das möglich ist, wird danach ein erneuter Versuch einer
		Strukturreduktion durchgeführt. Sollte eine Struktur gefunden worden
		sein, wird ihr Knoten vor dem nächsten Knoten in der Reihenfolge aus
		Schritt 2 betrachtet, indem die entsprechende Analysefunktion rekursiv
		ausgeführt wird.
       
        Konkret werden komplexe Bedingungen
		folgendermaßen untersucht: Zunächst wird überprüft, ob es sich bei
		der hypothetischen Bedingung um die Bedingung einer vorprüfenden
		Schleife handeln kann. Dies wird über eine Suche nach Backedges
		\footnote{
			Eine Backedge ist eine Kante, deren Ziel- ihren Ursprungsknoten dominiert.
		}
		, die den
		aktuellen Knoten zum Ziel haben, bewerkstelligt. Sollten solche Knoten
		vorhanden sein, werden alle Pfade (s. u), die diesen Knoten enthalten,
		aus der Analyse ausgeschlossen. Danach werden die möglichen Pfade für den
		Kontrollfluss ausgehend vom aktuellen Knoten berechnet. Nun wird ein Knoten mit
		besonderen Eigenschaften gesucht:
		\begin{enumerate}
			\item{Der Knoten ist in allen Pfaden vorhanden.}
			\item{
				Der Knoten ist dabei der relativ zu den Pfaden
				der erste Knoten, der die erste Eigenschaft erfüllt.}
			\item{Dieser Knoten ist zudem nicht der Anfangsknoten der Pfade.}
		\end{enumerate}
		Sollte es hingegen so sein, dass eine
		Schleifenbedingung betrachtet wird, werden nur die Pfade für die
		Bestimmung des Knotens herangezogen, die nicht den Schleifenkörper
		erreichen und dementsprechend keine Backedges enthalten, wie anfangs beschrieben.
		Der gefundene Knoten ist nicht mehr zur Struktur und Bedingung gehörig.
		Die Knoten, die vor ihm in den Pfaden auftreten, gehören hingegen
		dazu, werden anhand der Anzahl ihrer Nachfolger als Bedingungen oder 
		Code identifiziert. Die Knoten, die zur Bedingung gehören, also zwei Nachfolger
		besitzen, werden zu einem Knoten reduziert, der die komplexe Bedingung repräsentiert.
		Dies geschieht jedoch nur, wenn keiner der Knoten in der Bedingungstruktur,
		außer der Anfangsknoten, Vorgänger außerhalb der Struktur besitzt.
		Auf diese Weise wird verhindert, dass Teile von großen und komplexen Bedingungen
		als Strukturen reduziert werden, was die Reduktion sehr uneffizient und nicht
		gerade nützlich machen würde. Die Funktionalität, reduzierte Bedingungen weiter
		zu analysieren ermöglicht es, logische Verknüpfungen zwischen den Teilbedingungen
		zu erkennen und ein weiteres arbeitsintensives Detail des Reverse-Engineerings
		zu automatisieren.
	\item{\textbf{Analyse bereits reduzierter Bedingungen:}}
		Sollten im Rahmen der vorangehenden Analyseschritte Bedingungen reduziert worden sein,
		dann enthalten die entsprechenden Knoten nur die Einzelnen Elemente der Bedingung,
		sagen aber nichts über die logischen Verknüpfungen zwischen diesen aus. Aus diesem
		Grun werden alle Bedingungen einem weiteren Analyseschritt unterzogen, der es erlaubt,
		diese für den Nutzer wichtigen Informationen wiederherzustellen. Das Ergebnis dieses
		Prozesses lässt sich dann in die Ausgabe des Programms integrieren. Konkret sind dabe
		die folgenden Schritte notwendig:
		\begin{enumerate}
			\item{Es werden die Knoten bestimmt, die ausgeführt werden, wenn die Gesamtbedingung
				wahr oder falsch ist, und in einer dafür geeigneten Struktur gespeichert.}
			\item{Nun wird der Subgraph, der die Bedingung darstellt per Postorder traversiert.
				Für jeden Knoten wird überprüft, ob er den Anfang einer Struktur darstellt,
				die folgende Bedingungen erfüllt:
				\begin{itemize}
					\item{Er hat zwei Nachfolger.}
					\item{Beide Nachfolger besitzen ebenfalls zwei Nachfolger.}
					\item{Nachfolger $a$ ist Zugleich ein Nachfolger von
						Nachfolger $b$, der andere Nachfolger von $b$ ist
						gleichzeitig Nachfolger von $a$ und $a$ besitzt insgesamt
					        nur einen Nachfolger.}
				\end{itemize}
				Dabei werden die Äußeren Knoten, die im ersten Schritt bestimmt wurden, in
				die Analyse einbezogen. Wenn nun eine Struktur gefunden wird, wird anhand
				ihrer Orientierung entschieden, ob sie ein logisches ODER oder ein logisches
				UND darstellt. Dabei wird unter Orientierung folgendes verstanden: Der
				gemeinsame Nachfolger von $a$ und $b$ ist stets einer der im ersten Schritt
				bestimmten Knoten, die außerhalb der Bedingung liegen, zumal $a$ und $b$
				schon erkannte (komplexe) Teilbedingungen sein können. Wenn dieser gemeinsame
				Nachfolger bei einer wahren Gesamtbedingung ausgeführt wird, handelt es sich
				um ein logisches ODER, sonst um ein logisches UND. Sollte auf diese Weise eine
				Struktur erkannt werden, wird der aktuelle Knoten zusammen mit $a$ und $b$
				als ein gemeinsamer Knoten betrachtet, der dann einer erneuten Analyse nach
				den selben Prinzipien unterzogen wird. Gleichzeitig wird eine Beschreibung
				der Bedingung erstellt, die später in die Ausgabe integriert wird.
				Um dem Nutzer beim Lesen dieser Beschreibung keinen störenden
				Interpretationsspielraum zu lassen, wird ein Knoten innerhalb einer Bedingung
				so aufgefasst, dass seine elementare Bedingung dann wahr ist, wenn der
				bedingte Sprung an seinem Ende ausgeführt wird.}
		\end{enumerate}
\end{enumerate}

\subsection{Datenanalyse}
Bei diesem Modul liegt der Hauptaugenmerk auf der Möglichkeit, nicht nur
addressierte Speicherbereiche aus dem Code zu extrahieren, wie es z.B. in IDA Pro automatisch
geschieht, sondern auch aus dem Kontext der Speicheraufrufe auf die Funktion und Verwendung der
entsprechenden Speicherabschnitte zu schließen. Dabei sei erwähnt, dass solche Verfahren grundsätzlich nur
Heuristiken darstellen, da nicht jeder Speicherabschnitt, aus dem z.B. ein DWORD\footnote{4 byte (32
bit) großer Speicherabschnitt, meist ein Integer} gelesen wird, automatisch einer Integervariable
gleichgesetzt werden kann. So verwenden moderne Compiler die push\footnote{push alloziert Speicher
auf dem Stack und speichert dort den übergebenen Operanden}- und pop\footnote{pop entfernt den
zuletzt auf den Stack geschobenen Wert und speichert ihn im übergebenen Operanden}-Instruktionen
nicht mehr für Funktionsaufrufe, da diese zu viele Ressourcen verbrauchen. Stattdessen wird am
Anfang der Funktion mehr Speicher alloziert
\footnote{Das Betriebssystem \textit{alloziert} Speicher, um ihn einem Programm zur Verfügung zu
stellen. Dieser Prozess umfasst die Auswahl des Speicherbereichs, die Protokollierung des Vorgangs
und (implementationsbhängige) Aspekte, die es dem Programm ermöglichen, den Speicher tatsächlich
zu benutzen. Häufig ist mit diesem Begriff gemeint, dass Speicher, der dem Programm bereits zur Verfügung
steht, der allerdings frei ist, mit Daten belegt wird.}
, als für die lokalen Variablen notwendig, so dass
Parameter direkt in den Speicher geschrieben werden, bevor eine Funktion aufgerufen wird. Darüber
hinaus werden vermehrt sogenannte Canarys auf dem Stack hinterlegt, die sicherstellen sollen, dass
kein Stackoverflow stattfindet, bzw. Schäden am Stackframe direkt erkannt werden.
Dabei handelt sich um "freien" Speicher, der zwischen den lokalen
Variablen und dem für Parameter reservierten Platz alloziert wird und auf seine Integrität überprüft
wird, wenn die Funktion zurückkehrt. Auf diese Weise wird es viel schwieriger, die Rückkehradresse
durch Nutzereingaben zu verändern. Solche Canary-Werte können auch zwischen Arrays und anderen
Variablen eingefügt werden, was z.B. die Längenbestimmung von Arrays erschwert, zumal bei diesen
selten direkte Addressierungen der Elemente stattfinden, was es sehr schwierig macht, ihre Grenzen
genau zu bestimmen. Diese und andere Aspekte der Kompilierung, bei denen Informationen verlorengehen,
erschweren die Analyse und Wiederherstellung von Variablen mitunter sehr, was gleichermaßen für Menschen und
Programme gilt, wobei letztere weniger trugschlussanfällig zu sein scheinen, da sie das Analyseergebnis
schrittweise aufbauen, so dass mitunter Informationen nur unvollständig sind, jedoch nur selten objektiv falsch.

\subsection{Optimisierte Intergerdivision} Die derzeitige Umsetzung dieses Moduls basiert auf
praktischen Erfahrungen des Autors und von Nutzern der Reverse-Engineering-Stackexchange
Website \cite{stackexchange:4}.

\subsubsection{Herleitung des Algorithmus}
Aus den Ausführungen in \cite{stackexchange:4} folgt, dass folgende Formel zum Wiederherstellen
des Divisors benutzbar ist:
\begin{equation*}
	d = \ceil*{\frac{2^{bitness + rshift}}{x + 2^{bitness}}}
\end{equation*}
Die mathematische Herleitung ist verhältnismäßig verworren und nicht gerade klar, weswegen sie hier,
um Fehler zu vermeiden, nicht wiedergegeben wird. Dabei ist $d$ der gesuchte Divisor, $bitness$ ist
die Breite der verwendeten Register, $rshift$ die Anzahl Stellen, um die das Ergebnis verschoben wird
und $x$ die Konstante, mit der multipliziert wird.
Folglich bleibt es dem Code noch übrig diese Werte aus dem Assembler-Listing zu extrahieren. Um den
genauen Ablauf dieses Prozesses zu steuern, werden Grenzwerte festgelegt, wie weit die einzelnen
Instruktionen voneinander entfernt sein dürfen, damit der gegebene Codeabschnitt noch als Division
betrachtet wird. Sollten die automatisch verwendeten Werte nicht zum gewünschten Ergebnis führen,
kann der sogenannte \textit{interaktive Modus} verwendet werden, der den Nutzer die Konstanten von
Hand eingeben lässt und das Ergebnis der Berechnungen ausgibt. 

\section{Ausblick}
Trotz aller in den vorherigen Abschnitten beschriebenen Vorzügen des Projekts kann nicht von
einem kompletten und fehlerfreien Produkt gesprochen werden. Konkret stehen noch folgende Punkte aus:
\begin{itemize}
	\item{Die Dokumentation muss verbessert werden. Derzeit sind nahezu alle Teile des Programms
		mit Docstrings\footnote{Ein Docstring ist ein Kommentar, der bestimmten Formatvorgaben
		gerecht wird und eine Klasse, eine Funktion / Methode oder eine Variable allgemein beschreibt.
		}
		ausgestattet. Trotzdem ist schwer zu beurteilen, wie einfach oder schwer
		es für einen Außenstehenden mit hinreichenden Programmierkenntnissen ist, sich in den
		Code hineinzuversetzen und ihn komplett zu verstehen, zumal gerade die komplexen Algorithmen
		des Kontrollflussmoduls teilweise nicht in ihrer Funktionsweise beschrieben werden, was
		Modifikation und Verbesserung durch Dritte äußerst schwierig macht.
	}
	\item{Das CFG-Modul muss restrukturiert werden, um es stilistisch zu verbessern. Dies wurde
		bereits teilweise umgesetzt, ob tatsächlich weitere Umstrukturierungen notwendig sind,
		wird sich mit der Weiterentwicklung zeigen. Konkret ist damit gemeint, dass einige
		Zuständigkeiten von Klassen recht unkar definiert sind. Beispielweise werden Bedingungen
		zwar korrekt analysiert, allerdings findet dieser Prozess in der selben Klasse statt,
		deren Objekte Bedingungen representieren, was stilistisch fragwürdig erscheint. Die
		allgemeine Graphenanalyse wurde hingegen bereits in zwei Klassen, $Graph$ und $GraphAnalyzer$
		gespalten, auch um Erweiterungen zu vereinfachen und die Wiederverwedung des Codes
		zu ermöglichen.
	}
	\item{Gegebenenfalls sollte eine Unterstützung von \textit{break}, \textit{continue},
		\textit{goto} und \textit{return} in die Kontrollflussanalyse eingebunden werden.
		Generell kann derzeit von keiner Unterstützung für nicht wohlstrkturierte Programme
		gesprochen werden. Diese sind allerdings recht häufig anzutreffen, weswegen zumindest
		der Ansatz einer Unterstützung notwendig wäre, um die Verwendung des Projekts in der
		Praxis zu vereinfachen. Dies würde allerdings eine Änderung am Algorithmus des
		Kontrollflussanalysemoduls erfordern und ist daher noch nicht in Angriff genommen worden.
		Dabei liegt die grundsätziche Schwierigkeit darin, dass die verwendeten Algorithmen
		grundsätzlich wohlstrukturierte Programme erwarten und die Analyseergebnisse anderer
		Varianten der selben Algorithmen, die dieses Verhalten nicht besitzen, mit den vom
		Autor eingeführten Modifikationen, wie der Reduktion komlexer Bedinguungen gegebenenfalls
		interferieren würden.
	}
	\item{Derzeit unterstützt das Projekt keine Sprünge an zur Laufzeit berechnete Adressen. Gegebenenfalls
		sollte eine Unterstützung für diese Strukturen implementiert werden. Zur Zeit ist dies
		jedoch vermutlich nicht notwendig, da solche Sprünge aufgrund ihrer recht geringen Effizienz
		kaum von Compilern verwendet werden. Allerdings sind sie gerade zum Zwecke der Obfuskation
		sehr effizient, auch weil verschleierte Software, wie Malware, meist nicht sehr
		performanceabhängig ist, was wiederum ein entsprechendes Feature recht nützlich machen würde.
	}
	\item{Ein weiterer Aspekt des Programmverhaltens, der verbessert werden könnte, ist die Ausgabe bei
		(teilweise) unmöglicher Analyse des Kontrollflusses. Diese ist zwar seit Beginn des Projekts
		wesentlich verbessert worden, verfügt derzeit allerdings kaum über Kontrollmechanismen, die dem
		Nutzer helfen, den Erfolg der Analyse schnell und zuverlässig zu beurteilen. Aus diesem Grund
		wäre die Einführung einer relativen Metrik für die Analyseergebnisse durchaus sinnvoll. Ein
		entsprechendes System könnte zum Beispiel von der Anzahl verbliebener Knoten und Kanten im
		Graphen nach der Analyse ausgehen und dem Nutzer signalisieren, ob die Verwendung des Programms
		im Falle bestimmter Teile des Analysetargets überhaupt lohnenswert oder eine manuelle Analyse
		sinnvoller ist.
	}
	\item{Auch weitere Aspekte der Ausgabe und Nutzerinteraktion sind gegebenenfalls zu verbessern. Sinnvoll
		wären nicht nur mehr Eingabemöglichkeiten, wie bereits besprochen, sondern auch bessere
		Möglichkeiten zur Steuerung der Analyseparameter, zum Beispiel mehr Kommandozeilenparameter
		zum Ausschluss von einzelnen Funktionen, mehr Ausgabeformate, oder die Möglichkeit für den Nutzer,
		diese selbst zu modifizieren.
	}
	\item{Zudem sind derzeit große Teile des Kontrollflussmoduls noch nicht komplett stabil bei Eingaben, die
		nicht speziell zum Testen der Software entworfen wurden. Aus diesem Grund muss noch an der
		Weiterentwicklung und Stabilisierung des Systems gearbeitet werden, da Software dieses
		Komplexitätsniveaus häufg recht schwer zu testen und auf Fehler zu überprüfen ist.
	}
	\item{Gleichzeitig muss der Code weiterhin hohen stilistischen Standards gerecht werden, teilweise müssen
		diese noch weiter angehoben werden. Derzeit wird der Code des gesamten Projekts von $pylint$
		\footnote{$pylint$ ist ein sogenannter $Linter$, also ein Tool, dass den Code eines Projekts auf
			stilistische Mängel und allgemeine Lesbarkeit und Qualität des Codes untersucht.
		}
		mit etwa 9.72 / 10 Punkten bewertet. Es müssen einige Routineaufgaben in Methoden
		gekapselt werden, Klassen gegebenenfalls refactort werden etc.
	}
\end{itemize}
Gleichzeitig ist das Projekt durchaus recht weit seiner Fertigstellung fortgeschritten, unter
anderem, da alle Features einem sehr gründlichen System an Testläufen unterzogen wurden, die auch
auf Integration mit dem Restsystem ausgerichtet sind, was bedeutet, dass Tests, die einzelnen
Bausteinen eines Moduls gelten so konzipiert sind, dass aus ihren Ergebnissen auf das Verhalten des
Systems in veränderten Kontexten geschlossen werden kann. Auf diese Weise wird verhndert, dass
einzelne Fehler in bestimmten Eingabekonfigurationen auftreten können. Dies ist besonders für das
Kontrollflussanalysemodul wichtig, da dieses mit Abstand am komplexesten ist, weswegen auch
zukünftige Erweiterungen sehr streng auf ihre Interoperabilität geprüft werden müssen.

\bibliography{first}
\bibliographystyle{unsrt}

\end{document}
