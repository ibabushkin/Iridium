% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode

% This is a simple template for a LaTeX document using the "article" class.
% See "book", "report", "letter" for other types of document.

\documentclass[11pt]{article} % use larger type; default would be 10pt

\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)
\usepackage{natbib}
\usepackage{multicol}

%%% Examples of Article customizations
% These packages are optional, depending whether you want the features they provide.
% See the LaTeX Companion or other references for full information.

%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{a4paper} % or letterpaper (US) or a5paper or....
% \geometry{margin=2in} % for example, change the margins to 2 inches all round
% \geometry{landscape} % set up the page for landscape
%   read geometry.pdf for detailed page layout information

\usepackage{graphicx} % support the \includegraphics command and options

% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
% These packages are all incorporated in the memoir class to one degree or another...

%%% HEADERS & FOOTERS
\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\pagestyle{fancy} % options: empty , plain , fancy
\renewcommand{\headrulewidth}{0pt} % customise the layout...
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\thepage}\rfoot{}

%%% SECTION TITLE APPEARANCE
\usepackage{sectsty}
\allsectionsfont{\sffamily\mdseries\upshape} % (See the fntguide.pdf for font help)
% (This matches ConTeXt defaults)

%%% ToC (table of contents) APPEARANCE
\usepackage[nottoc,notlof,notlot]{tocbibind} % Put the bibliography in the ToC
\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents
\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!

%%% END Article customizations

%%% The "real" document content comes below...
\usepackage{dirtree}
\columnseprule0.01pt
\columnsep7mm
\usepackage[cm]{fullpage}

\usepackage{amsmath, amssymb, amstext, amsfonts, mathrsfs, url}
\usepackage{mathtools}
\DeclarePairedDelimiter\ceil{\lceil}{\rceil}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}
%\usepackage{mathtools, mathpazo}
%\usepackage{url}
%\usepackage[colorlinks=true, linkcolor=black, citecolor=black, urlcolor=blue, pdftex,pagebackref]{hyperref}
\renewcommand{\bibname}{Bibliotheksreferenz}
\renewcommand{\contentsname}{Inhalt}
\renewcommand{\figurename}{Fig.}

\title{Besondere Lernleistung - Dokumentation}
\author{Inokentiy Babushkin}
%\date{} % Activate to display a given date or no date (if empty),
         % otherwise the current date is printed 

\begin{document}
\maketitle
\tableofcontents
%\begin{multicols}{2}

\section{Grundlagen}
Die vorliegende Besondere Lernleistung hat das Ziel, ein Softwareprojekt
umfassend zu planen und umzusetzen.
Dieses Dokument beschreibt Zielsetzung, Aufbau und konkrete Aspekte der Umsetzung,
wobei der Hauptaugenmerk auf Algorithmen, Vorgehensweisen und Designentscheidungen liegen wird.

\section{Problemantik}
Das erklärte Ziel dieses Projekts ist es, ein Programm zu schreiben, welches
es ermöglicht, Software teilweise automatisch zu dekompilieren. Da moderne Software immer kompexer
wird, wird es entsprechend schwieriger, in binärer Form vorliegende Softwareprodukte so umzuformen,
dass sie ein tiefgehendes Verständnis und darauf basierende Modifikation erlauben. Aus diesem Grund
werden bereits seit einiger Zeit Ansätze unternommen, diesen Vorgang so weit wie möglich zu automatisieren.
Die Gründe dafür sind, dass die Wiederherstellung von Kontrollfluss, Variablen, Funktionen und
weiteren HLL\footnote{High Level Language, also Programmiersprache mit hohem Abstraktionsgrad, bekannte
Beispiele für kompilierte HLL's sind C, C++ und Java}-spezifischen Strukturen ohnehin nicht komplett
möglich ist, das Ergebnis also bloß eine Annäherung an den Originalcode sein kann. Gleichzeitig ist
der Vorgang - von Menschen durchgeführt - sehr zeitraubend, erfordert viel Fachwissen und
detaillierte Kenntnisse über die Funktionsweise der zu untersuchenden Software. All dies lässt es
vorteilhaft erscheinen, die recht "undankbare" Aufgabe des Reverse-Engineerings auf ein
automatisiertes System zu verlagern, vorausgesetzt das Ziel ist der komplette Quellcode des zu
untersuchenden Produkts. Diese Systeme beruhen auf verschiedenen theoretischen Ansätzen,
Programme zu beschreiben und Muster in ihnen zu erkennen, die in ihrer Komplexität stark variieren.
Dabei sind mehrer Programmkategorien zu unterscheiden:
\begin{enumerate}
	\item{Decompiler, deren Ziel es ist, ein Programm
		komplett wiederherzustellen, die meistens eine non-interaktive Arbeitsweise erfordern.}
	\item{Programme, die bestimmte Aspekte des zu analysierenden Codes wiederherstellen oder eine Art
		Anleitung für den Benutzer generieren, wie er am besten verfährt, um ein umfassendes Verständnis
		vom zu untersuchenden System zu erlangen.}
\end{enumerate}
Das Resultat der vorliegenden Besonderen Lernleistung ist in
die zweite Kategorie einzuordnen, wobei es aus drei Submodulen besteht, die unabhängig arbeiten und
verschiedene Bereiche des Analysegebiets abdecken. Die Entscheidung, keinen kompletten Decompiler
anzustreben wurde aus mehreren Gründen getroffen: Zum Einen ist automatisierte Programmanalyse recht
genau, kann jedoch trotz allem Fehler erzeugen. Deswegen ist es  deutlich günstiger, eine
transparente Arbeitsweise und eine enge Zusammenarbeit zwischen Benutzer und System zu ermöglichen,
da auf diese Weise Fehler deutlich schneller gefunden und behoben werden können. Zum Anderen sind
viele Funktionen und Features in einem solchen Projekt machbar und können auch mehr oder weniger
einfach integriert werden, obwohl sie kaum Nutzen erbringen, da das Verhältnis zwischen
Ergebnisqualität und Programmkomplexität zu schlecht ist. Folglich ist es sinnvoll, sich auf die
wesentlichen Aufgabenstellungen zu beschränken, so dass der Aufwand in Relation zum erbrachten
Nutzen steht.

\section{Zielsetzung} Wie bereits im letzten Abschnitt erwähnt, handelt es sich bei diesem Projekt
um ein Analyseframework zur (teilweisen) Dekompilierung von Software, wobei auch eine Vielzahl
weiterer Reverse-Engineering\footnote{Der Prozess, Software oder andere technische Systeme in ihrem
Aufbau und Verhalten zu analysieren, um sie zu reproduzieren, zu modifizieren oder das dabei
gewonnene Wissen weiterzuverwenden, z.B. um Anti-Malware Routinen zu entwickeln}-Aufgaben mit seiner
Hilfe vereinfacht werden können. Dabei wird die Ausgabe von IDA Pro\footnote{Ein Disassembler, der
viele verschiedene Eingabeformate unterstützt und nicht nur interaktive Arbeit ermöglicht, sondern
auch Assemblerlistings etc. generieren kann.} als Grundlage der Analyse verwendet. Dies mag zuerst
als Nachteil erscheinen, da es den Arbeitsverlauf des Nutzers einschränkt, wobei bedacht werden
muss, dass der Analyse- und Input-Parsing-Code strikt getrennt wird, was es ermöglicht, andere
Eingabeformate mithilfe weiterer Module zu akzeptieren, ohne Änderungen in der Programmmechanik
umsetzen zu müssen. Die Umsetzung des Moduls für IDA wurde zuerst durchgeführt, da diese keine
wesentliche Verarbeitung der Eingabe erfordert, um die für die Programmmechanik notwendigen
Informationen bereitzustellen, weswegen ein solches Modul sich besonders gut als
Demonstrationsbeispiel eignet.

\subsection{Projektstruktur} Um eine Erweiterung im Sinne des vorhergehenden Abschnittes zu
gewährleisten, ist es besonders wichtig, eine sinnvole Aufteilung des Codes in einzelne Module
durchzuführen. Auch sonst gehört es zum "guten Stil", Programme in Sinneinheiten zu koppeln.
Wie bereits erwähnt, wird deswegen das Projekt grob in zwei zentrale Teile unterteilt: Die
tatsächlichen Analysemodule, die alle eine Eingabe in einem bestimmten Format erwarten, und Module,
die den Zweck erfüllen, Daten in diesem Format bereitzustellen und diese aus verschiedenen
Eingabeformaten generieren. Die Ordnerstruktur sieht folgendermaßen aus und wird primär verwendet,
um das Projekt zu strukturieren. An der Wurzel der Ordnerstruktur liegen die einzelnen Module, die
Eingabe verarbeiten und in ein Format umwandeln, welches von den Analysemodulen verwendet wird.
Diese wiederum befinden sich innerhalb des Moduls \textit{defines}, welches pro Submodul einen
Unterordner besitzt. Jedes dieser Submodule kann sowohl alleinsthend, also als Script, als auch als
Modul benutzt, also imporiert werden. Auf diese Weise sind die einzelnen Komponenten
Kommunikationsfähig, jedoch trotzdem nur wenn notwendig voneinander abhängig, so dass Erweiterung
und Modifikation sehr einfach zu bewerkstelligen ist.
%\newpage
\begin{figure}[h!]
%	\centering
		\dirtree{%
		.1 Iridium (Eingabeparsing-Module).
		.2 defines (Behälter für Submodule).
		.3 cfg (Kontrollflussanalyse).
		.3 data (Bestimmung von Variablen etc).
		.3 div (Analyse optimisierter Divisionen).
		.3 util (Parent-Klassen etc).
		.2 docs (Dokumentation, wie dieses Dokument).
		.2 tests (Testdateien und -eingaben).
		}
	\caption{Die Ordnerstruktur des Projekts}
\end{figure}
\subsection{Kontrollflussanalyse} \label{sec:CFA} Einer der zentralen Aspekte moderner wie
historischer Programme ist die Möglichkeit, Code in Abhängigkeit von Bedingungen ausführen zu
können. Die am weitesten bekannten Beispiele sind verschiedene Arten Bedingter Anweisungen, Schleifen u.ä. Diese
Beziehungen zwischen den einzelnen Teilen des Codes wiederherstellen zu können ist eine der
Hauptaufgaben dieses Projekts. Dabei ist diese Zielsetzung keineswegs trivial, zumal Strukturen
linear und nicht linear verschachtelt\footnote{Strukturen in einem CFG bestehen in der Regel aus
zwei oder Mehr Teilen, wobei sogenannte primitive Teile einfache Knoten des Kontrollflussgraphen
sind. In der Praxis können jedoch auch nich-primitive Teile, also andere Strukturen an dieser Stelle
auftreten. Wenn eine Struktur komplett eine andere Teilstruktur ersetzt, wird der Begriff "linear
verschachtelt" verwendet. Gleichzeitig ist es möglich, dass Strukturen verschiedener Ebenen den
gleichen Knoten verwenden, was als nichtlineare Verschachtelung bezeichnet wird. Beide Begriffe sind
vom Autor selbst geprägt.} sein können etc. Als Grundlage des verwendeten Algorithmus diente die in
\cite{structural_analysis:1} beschriebene Vorgehensweise, die jedoch um fortgeschrittenere
Bedingungsanalyse erweitert wurde. Dies war notwendig, da z.B. Bedingte Anweisungen nicht nur eine,
sondern mehrere über logische Operatoren verknüpfte Bedingungen besitzen können. Dies führt dazu,
dass Strukturen erst erkannt werden können, wenn diese komplexen Bedingungen reduziert wurden.

\subsection{Analyse von Variablen und anderen Daten} Daten bilden neben dem Code selbst den zweiten
Hauptbestandteil eines Programms, weswegen das Layout der Variablen, Argumente/Parameter und
Rückgabewerte von großem Interesse für den Reverse-Engineer sein sollte. Gleichzeitig sind Arrays
und Variablen nicht immer trivial zu erkennen, weswegen sich einige einfache, dennoch effektive
Heuristiken anbieten, die in diesem Projekt umfassend realisiert wurden. Die eigentliche Problematik
liegt darin, dass Compiler nicht alle Daten auf dem Stack ablegen, aber gleichzeitig den Stack nicht
nur zum Speichern lokaler Variablen verwenden. Folglich muss unterschieden werden, welche
Speicherbereiche Variablen abbilden, welchen Datentyp diese besitzen und ob diese gegebenenfalls
Pointer darstellen. Aufgrund der oben beschrieben Umstände handelt es sich bei den
Analyseergebnissen nur um eine Annäherung an die Realität, beispielsweise verwenden optimisierende
Compiler zum Speichern lokaler Variablen häufig CPU-Register
\footnote{Ein Speicherbereich, der direkt an die CPU angeschlossen ist und sehr viel schnelleren
Zugriff erlaubt, allerdings in seiner Größe und Anzahl begrent ist. Jede Variation der
Assemblersprache definiert eine bestimmte Anzahl sogenannter Universalregister, die auf bestimmte
Weise benannt werden und spezifisch für die jeweilige Prozessorarchitektur sind. Für die
x86-Prozessorarchitektur sind dies EAX, EBX, ECX, EDX, ESI, EDI, EBP und ESP. Diese sind 32 Bit
breit, fassen also 4 Byte und können auch teilweise angesprochen werden. Dabei sind EBP und ESP mit
einer besonderen Bedeutung versehen: Sie zeigen auf Anfang und Ende des aktuellen Stackframes und
können nicht vom Programm anderweitig benutzt werden.}
, die von dem vorliegenden
Modul nicht berücksichtigt werden, da auch so viel Arbeit in zweifelhafte Ergebnisse investiert
werden würde, zumal Register auch für Arithmetik und Argumentübergabe verwendet werden können.

\subsection{Behandlung von Integerdivision} Die Integerdivision ist, ebenso wie die Modulodivision,
eine verhältnismäßig häufig verwendete Rechenarten in Programmen, bzw. ist meistens von zentraler
Bedeutung für die Resultate der Ausführung. Allerdings sind diese Operationen auch sehr
ressourcenintensiv, weswegen seit den späten 1990er Jahren die entsprechenden Prozessorinstruktionen
kaum noch verwendet wurden, zumal die Division durch Konstanten durch Multiplikation und Shifts
ersetzt werden kann, was deutlich schnellere Ausführung ermöglicht\cite{division:2}, wodurch
gerade Schleifen beschleunigt werden können. Gleichzeitig wird dadurch die eigentliche
Bedeutung des Codes verschleiert, was zu Hürden bei der Analyse durch Menschen führt. Auch hier ist
eine automatisierte Lösung also naheliegend, zudem noch nicht allgemein verbreitet\footnote{Eine
Recherche des Autors ergab, dass z.B. der Hex-Rays Decompiler "ohne Probleme" mit solchen
Konstrukten umgehen kann, da dieser allerdings ein komerzielles und noch dazu recht teures Produkt
darstellt, kann vermutlich nicht vonn allgemeiner Verbreitung gesprochen werden.}. Gleichzeitig ist
die Berechnung des Divisors aus den vorliegenden Konstanten durchaus möglich und ausreichend
beschrieben bzw. erklärt\cite{stackexchange:4}.

\section{Umsetzung} Im Folgenden werden die verwendeten Algorithmen im Detail beschrieben,
auch Informationen zur Implementation sind hier wiedergegeben.

\subsection{Kontrollflussanalyse} Wie bereits in Abschnitt \ref{sec:CFA} beschrieben, ist die
Kontrollflussanalyse eine verhältnismäßig komplexe Aufgabe, da das zu untersuhende Objekt eine
Vielzahl an Formen annehmen kann, beispielsweise arrangieren Compiler den generierten Assemblercode
nicht immer auf eine direkt erwartete Weise. Aus diesem Grund wurde im Laufe der Planung
vorliegender Software sehr schnell klar, dass die Analyse des Programms auf Codeebene nicht
erfolgreich sein kann, da die Strukturen kaum noch erkannt werden, wenn sie verschachtelt vorliegen.
Eine Untersuchung der Fachliteratur ließ erkennen, dass die Analyse des Kontrollflusses deutlich
effektiver ist, wenn dieser als Graph betrachtet wird. Per Definition ist ein Kontrollfussgraph
(CFG) ein gerichteter Graph \(G\), der über eine Menge Knoten \(V\), eine Menge gerichteter Kanten
\(E\) und einen Wurzelknoten \(r\) mit \(r \in V\) verfügt \cite{wiki1:3}. \[G(V,E,r)\] Aufgrund der
Konzeption von Codeflusssteuerung auf Machienenebene kann zudem davon ausgegangen werden, dass jeder
Knoten 0 bis 2 Nachfolger hat, allerdings eine beliebige Anzahl Vorgänger, wobei nur der
Wurzelknoten keinen Vorgänger haben darf. Die Verwendung einer solchen Datenstruktur zur
Representation des Programms hat eine Reihe von Vorteilen, wobei die wichtigsten eine deutlich
einfachere Analyse möglicher Pfade vom Start- zum Endknoten und die Unabhängigkeit vom Codelayout
durch den Compiler im Speicher sind.  Die Software geht bei der Analyse folgendermaßen vor: Die
Knoten werden nacheinander bearbeitet, wobei die Reihenfolge der Postorder-Traversierung des
Tiefensuchbaumes des Graphen entspricht. Ist der aktuelle Knoten der Anfang einer simplen Struktur,
die es zu erkennen gilt, werden die dazugehörigen Knoten und Kanten extrahiert und durch einen
Knoten ersetzt, der Informationen über die enthaltene Struktur speichert, und welcher als nächstes
analysiert wird. Sobald alle Knoten auf diese Weise behandelt wurden und alle notwendigen
Graphenreduktionen stattgefunden haben, können die Informationen über erkannte Schleifen,
Verzweigungen etc. in strukturierter Form dargestellt werden. Das aktuelle Featureset umfasst vor-
und nachprüfende Schleifen, Verzweigungen mit beliebig vielen alternativen Pfaden und alle dem Autor
bekannten Möglichkeiten der Verschachtelung. Seit neuestem werden auch komplexe Bedingungen in allen
möglichen Kontexten erkannt und reduziert, was ein Novum darstellt, da der dazu verwendete
Algorithmus bs jetzt nicht in der dem Autor bekannten Fachliteratur beschrieben wurde. Sprungbefehle
wie goto, break, continue und in manchen Fällen return werden aufgrund der Herangehensweise nicht
erkannt, wobei eine entsprechende Änderung in zukünftigen Versionen durchaus möglich ist.

\subsubsection{Algorithmusbeschreibung und Implementationsdetails} Das Programm lässt sich in
mehrere Abschnitte unterteilen, die sich verschiedenen Phasen der Analyse zuordnen lassen:
Generieren des Graphen und die Analyse des Graphen selbst, die sich wiederum in zwei Abschnitte
unterteilbar ist: Bestimmung der Analysereihenfolge der Knoten und die Analyse jedes einzelnen
Knotens nach dieser Reihenfolge.
\begin{enumerate}
	\item{\textbf{Generierung des Graphen aus Assemblercode:}}
		Die Unterteilung des Assemblerlistings erfolgt an sogenannten Trennpunkten
		zwischen Codezeilen.
		Diese erfüllen mindstens eine der nachfolgenden Bedingungen:
		\begin{enumerate}
			\item{Die vorherige Instruktion ist ein bedingter oder unbedingter Sprung.}
			\item{Die nachfolgende Instruktion mit einem Label versehen,
				also Ziel eines oder mehrer Sprünge.}
			\item{Die nachfolgende Instruktion ist ein Vergleich.}
		\end{enumerate}
		Wenn auf diese Weise alle Knoten des Graphen bestimmt sind, werden mögliche Übergänge von
		Knoten zu Knoten, also Kanten des Graphen, generiert, die \textit{aktiv} oder
		\textit{passiv} sein können. \textit{Aktive} Kanten sind Kanten, die
		tatsächlich ausgeführten Sprüngen entsprechen, während \textit{passive}
		Kanten keinem Sprung entsprechen, sondern einem Übergang zum nächsten Befehl,
		der in einem anderen Knoten liegt.
	\item{\textbf{Bestimmung der Analysereihenfolge für die Knoten des Graphen:}} Hierfür wird
		zuerst der Tiefensuchbaum des in Schritt 1 erstellten Graphen berechnet.\footnote{
			Ein Tiefensuchbaum oder Depth-First-Search-Tree (DFS-Tree) wird generiert,
			indem ein Graph per Tiefensuche traversiert wird und die dabei besuchten
			Knoten auf eine bestimmte Weise in einem  Baum abgelegt werden. Eine genauere
			Beschreibung des Algorithmus kann unter \cite{wiki2:5} eingesehen werden.
		}
		Dieser wird dann per Postorder\footnote{
			Die Postorder-Traversierung ist eine Methode, für einen Baum eine eindeutige
			Reihenfolge der Knoten zu bestimmen. Dabei handelt es sich um einen
			rekursiven Algorithmus, der auf jeden Knoten Teilbaum im Baum angewendet
			wird. Dabei wird das Ergebnis der Postorder-Traversierung der Kindknoten
			zuerst an die Liste der Knoten angefügt, danach der eigentliche Knoten. Auf
			diese Weise wird der ganze Baum durchlaufen, wobei die Wurzel immer das
			letzte Element der Ergebnismenge darstellt. Mehr Details sind unter
			\cite{wiki3:6} zu finden.
		}
		traversiert. Die sich dabei ergebende Reihenfolge ist
		für die Analyse sehr gut geeignet, da auf diese Weise die innersten Strukturen
		zuerst erkannt und reduziert werden, so dass kein Knoten, der am Anfang einer
		Struktur steht, analysiert wird, während der Graph eine Erkennung der entsprechenden
		Struktur unmöglich macht.
	\item{\textbf{Analyse der einzelnen Knoten nach der in Schritt 2 bestimmten Reihenfolge:}}
		Zuerst wird überprüft, ob der aktuelle Knoten Anfang einer simplen
		Struktur sein kann. Als simple Strukturen werden alle elementaren
		Konstrukte bezeichenet, die von Hochsprachen definiert werden: vor-
		und nachprüfende Schleifen, lineare Abfolgen beliebiger Strukturen
		und if-then / if-then-else -strukturen, also Bedingte Anweisungen.
		Das Programm unterscheided die folgenden Strukturen:
		\begin{enumerate}
			\item{\textit{if-then:}}
				Eine bedingte Anweisung, ohne else-Klausel.
			\item{\textit{if-then-else:}}
				Eine bedingte Anweisung, mit else-Klausel.\footnote{
					Bedingte Anweisungen, die mindestens ein \textit{else if}
					enthalten, werden als verschachtelte \textit{if-then-else}
					bzw. \textit{if-then}-Srukturen angesehen, da dies die
					Analyse deutlich vereinfacht und zudem für den Compiler
					keinen Unterschied darstellt.
				}
			\item{\textit{while-loop:}}
				Eine vorprüfende Schleife, entspricht sowohl der
				while- als auch der for-Schleife, da diese im Grunde
				genommen identisch vom Compiler behandelt werden.
			\item{\textit{do-loop:}}
				Eine nachprüfende Schleife.
			\item{\textit{condition:}}
				Eine komplexe Bedingung.
			\item{\textit{block:}}
				Eine beliebig lange, lineare Abfolge von sowohl
				primitiven, als auch Struktur-Knoten, ausgenommen
				Bedingungen.\footnote{
					Tatsächlich kann es vorkommen, dass Bedingungen direkt in
					einem Block platziert werden, wenn dieser Block der einzige
					Knoten in einer nachprüfenden Schleife ist.
					Dies liegt an der Art, auf die nachprüfende Schleifen
					erkannt werden und wird \textit{nicht} als Bug angesehen.
				}
		\end{enumerate}
		Ist eine solche Sruktur aufzufinden, werden ihre Teile aus dem Graphen entfernt und ein
		Knoten wird in den Graphen an entsprechender Stelle eingefügt, der
		die Informationen über sie speichert. Sollte dies jedoch nicht der Fall
		sein, wird überprüft, ob zuerst eine komplexe Bedingung reduziert
		werden kann, falls das möglich ist, wird danach ein erneuter Versuch einer
		Strukturreduktion durchgeführt. Sollte eine Struktur gefunden worden
		sein, wird ihr Knoten vor dem nächsten Knoten in der Reihenfolge aus
		Schritt 2 betrachtet, indem die entsprechende Analysefunktion rekursiv
		ausgeführt wird. Konkret werden komplexe Bedingungen
		folgendermaßen untersucht: Zunächst wird überprüft, ob es sich bei
		der hypothetischen Bedingung um die Bedingung einer vorprüfenden
		Schleife handeln kann. Dies wird über eine Suche nach Backedges, die den
		aktuellen Knoten zum Ziel haben, bewerkstelligt. Sollten solche Knoten
		vorhanden sein, werden alle Pfade (s. u), die diesen Knoten enthalten,
		aus der Analyse ausgeschlossen. Danach werden die möglichen Pfade für den
		Kontrollfluss ausgehend vom aktuellen Knoten berechnet.  Nun wird
		nach einem Knoten mit speziellen Eigenschaften gesucht: Dem ersten
		gemeinsamen Knoten aller gefundenen Pfade, der nicht dem
		Anfangsknoten entspricht. Sollte es hingegen so sein, dass eine
		Schleifenbedingung betrachtet wird, werden nur die Pfade für die
		Bestimmung des Knotens herangezogen, die nicht den Schleifenkörper
		erreichen und dementsprechend keine Backedges enthalten, wie anfangs beschrieben.
		Der gefundene Knoten ist nicht mehr zur Struktur und Bedingung gehörig.
		Die Knoten, die vor ihm in den Pfaden auftreten, gehören hingegen
		dazu, werden anhand der Anzahl ihrer Nachfolger als Bedingungen oder 
		Code identifiziert. Die Knoten, die zur Bedingung gehören, also zwei Nach-
		folger besitzen, werden zu einem Knoten reduziert, der die komplexe Bedingung
		representiert.
		Dies geschieht jedoch nur, wenn keiner der Knoten in der Bedingungstruktur,
		außer der Anfangsknoten, Vorgänger außerhalb der Struktur besitzt.
		Auf diese Weise wird verhindert, dass Teile von großen und komplexen Bedingungen
		als Strukturen reduziert werden, was die Reduktion sehr unperformant und nicht
		gerade nützlich machen würde. Derzeit ist Code in der Entwicklung, der die Analyse
		von bereits reduzierten Bedingungen übernehmen soll. Dies würde es ermöglichen,
		logische Verknüpfungen zwischen den Teilbedingungen zu erkennen und ein weiteres
		arbeitsintensives Detail des Reverse-Engineerings zu automatisieren.
\end{enumerate}

\subsection{Datenanalyse}
Bei diesem Modul liegt der Hauptaugenmerk auf der Möglichkeit, nicht nur
addressierte Speicherbereiche aus dem Code zu extrahieren, wie es z.B. in IDA Pro
geschieht, sondern auch aus dem Kontext der Speicheraufrufe auf die Funktion der entsprechenden
Speicherabschnitte zu schließen. Dabei sei erwähnt, dass solche Verfahren grundsätzlich nur
Heuristiken darstellen, da nicht jeder Speicherabschnitt, aus dem z.B. ein DWORD\footnote{4 byte (32
bit) großer Speicherabschnitt, meist ein Integer} gelesen wird, automatisch einer Integervariable
gleichgesetzt werden kann. So verwenden moderne Compiler die push\footnote{push alloziert Speicher
auf dem Stack und speichert dort den übergebenen Operanden}- und pop\footnote{pop entfernt den
zuletzt auf den Stack geschobenen Wert und speichert ihn im übergebenen Operanden}-Instruktionen
nicht mehr für Funktionsaufrufe, da diese zu viele Ressourcen verbrauchen. Stattdessen wird am
Anfang der Funktion mehr Speicher alloziert als für die lokalen Variablen notwendig, so dass
Parameter direkt in den Speicher geschrieben werden, bevor eine Funktion aufgerufen wird. Darüber
hinaus werden vermehrt sogenannte Canarys auf dem Stack hinterlegt, die sicherstellen sollen, dass
kein Stackoverflow stattfindet, bzw. Schäden am Stackframe direkt erkannt werden.
Dabei handelt sich um "freien" Speicher, der zwischen den lokalen
Variablen und dem für Parameter reservierten Platz alloziert wird und auf seine Integrität überprüft
wird, wenn die Funktion zurückkehrt. Auf diese Weise wird es viel schwieriger, die Rückkehradresse
durch Nutzereingaben zu verändern. Solche Canary-Werte können auch zwischen Arrays und anderen
Variablen eingefügt werden, was z.B. die Längenbestimmung von Arrays erschwert, zumal bei diesen
selten direkte Addressierungen der Elemente stattfinden, was es sehr schwierig macht, ihre Grenzen
genau zu bestimmen. Diese und andere Aspekte der Kompilierung, bei denen Informationen verlorengehen,
erschweren die Analyse und Wiederherstellung mitunter sehr, was gleichermaßen für Menschen und
Programme gilt, wobei letztere weniger trugschlussanfällig zu sein scheinen, da sie das Analyseergebnis
schrittweise aufbauen, so dass mitunter Informationen nur unvollständig sind, jedoch nur selten objektiv falsch.

\subsection{Optimisierte Intergerdivision} Die derzeitige Umsetzung dieses Moduls basiert auf
praktischen Erfahrungen des Autors und von Nutzern der Reverse-Engineering-Stackexchange
Website \cite{stackexchange:4}.

\subsubsection{Herleitung des Algorithmus}
Aus den Ausführungen in \cite{stackexchange:4} folgt, dass folgende Formel zum Wiederherstellen
des Divisors benutzbar ist:
\begin{equation*}
	d = \ceil*{\frac{2^{bitness + rshift}}{x + 2^{bitness}}}
\end{equation*}
Die mathematische Herleitung ist verhältnismäßig verworren und nicht gerade klar, weswegen sie hier,
um Fehler zu vermeiden, nicht wiedergegeben wird. Dabei ist $d$ der gesuchte Divisor, $bitness$ ist
die Breite der verwendeten Register, $rshift$ die Anzahl Stellen, um die das Ergebnis verschoben wird.
Folglich bleibt es dem Code noch übrig diese Werte aus dem Assebly-Listing zu extrahieren. Um den
genauen Ablauf dieses Prozesses zu steuern, werden Grenzwerte festgelegt, wie weit die einzelnen
Instruktionen voneinander entfernt sein dürfen, damit der gegebene Codeabschnitt noch als Division
betrachtet wird. Sollten die automatisch verwendeten Werte nicht zum gewünschten Ergebnis führen,
kann der sogenannte \textit{interaktive Modus} verwendet werden, der den Nutzer die Konstanten von
Hand eingeben lässt und das Ergebnis der Berechnungen ausgibt. 

\section{Ausblick}
Trotz aller in den vorherigen Abschnitten beschriebenen Vorzügen des Projekts kann nicht von
einem kompletten und fehlerfreien Produkt gesprochen werden. Konkret stehen noch folgende Punkte aus:
\begin{itemize}
	\item{Die Dokumentation muss verbessert werden.}
	\item{Die Analyse reduzierter Bedingungen muss umgesetzt werden.}
	\item{Das CFG-Modul muss restrukturiert werden, um es stylistisch zu verbessern.}
	\item{Gegebenenfalls sollte eine Unterstützung von \textit{break}, \textit{continue},
		\textit{goto} und \textit{return} in die Kontrollflussanalyse eingebunden werden.}
\end{itemize}

\bibliography{first}
\bibliographystyle{unsrt}
%\end{multicols}

\end{document}
